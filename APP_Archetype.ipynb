{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **STEP 1. Run the following cell to import the necessary libraries the first time you launch the runtime**\n",
        "It may takes some seconds"
      ],
      "metadata": {
        "id": "A-QGmAUC7fQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "!pip install --upgrade numpy\n",
        "import subprocess\n",
        "\n",
        "def install(package):\n",
        "    subprocess.check_call([\"pip\", \"install\", package])\n",
        "\n",
        "# Check if pyLDAvis is installed\n",
        "try:\n",
        "    import pyLDAvis\n",
        "except ImportError:\n",
        "    # If pyLDAvis is not installed, install it\n",
        "    install(\"pyLDAvis\")\n",
        "\n",
        "# Check if gensim is installed\n",
        "try:\n",
        "    import gensim\n",
        "except ImportError:\n",
        "    # If gensim is not installed, install it\n",
        "    install(\"gensim\")\n",
        "\n",
        "# Check if spacy is installed\n",
        "try:\n",
        "    import spacy\n",
        "except ImportError:\n",
        "    # If spacy is not installed, install it\n",
        "    install(\"spacy\")\n",
        "\n",
        "# Check if matplotlib is installed\n",
        "try:\n",
        "    import matplotlib\n",
        "except ImportError:\n",
        "    # If matplotlib is not installed, install it\n",
        "    install(\"matplotlib\")\n",
        "\n",
        "# Check if seaborn is installed\n",
        "try:\n",
        "    import seaborn\n",
        "except ImportError:\n",
        "    # If seaborn is not installed, install it\n",
        "    install(\"seaborn\")\n",
        "\n",
        "# Check if the en_core_web_md model for spacy is installed\n",
        "try:\n",
        "    import en_core_web_md\n",
        "except ImportError:\n",
        "    # If the model is not installed, download it using spacy\n",
        "    !python -m spacy download en_core_web_md -qq\n",
        "\n",
        "# Check if gensim is installed\n",
        "try:\n",
        "    import gensim\n",
        "except ImportError:\n",
        "    install(\"gensim\")\n",
        "\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.models import LdaMulticore\n",
        "from gensim.models import CoherenceModel\n",
        "from gensim.models import LdaModel\n",
        "#from gensim.summarization import summarize\n",
        "\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "import spacy\n",
        "import pyLDAvis.gensim_models\n",
        "pyLDAvis.enable_notebook()# Visualise inside a notebook\n",
        "import en_core_web_md\n",
        "import random\n",
        "import numpy as np\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display"
      ],
      "metadata": {
        "id": "O3sGL6ETbg4s",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **STEP 2. Run the following cell to upload the file and select model parameters**\n",
        "\n",
        "Note: the excel file must contain a header row that is not considered in the code. All the course/job description must be one per row. No limit in words per row (at least 100 words recomended). At least 100 courses/job description are recomended."
      ],
      "metadata": {
        "id": "8aaPyhlv7uNs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "oUn8Nd5SFTNm"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# FUNCTION to preprocess data\n",
        "def preprocess_data(df):\n",
        "    # Tags I want to remove from the text\n",
        "    removal = ['ADJ','ADV', 'AUX', 'INTJ', 'PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE', 'NUM', 'SYM']\n",
        "\n",
        "    # Words I want to remove (must be in lowercase and in the singular form)\n",
        "    cls=spacy.util.get_lang_class('en')\n",
        "    cls.Defaults.stop_words |= {'exercise', 'exercises', 'hour', 'hours', 'analysis', 'system', 'systems', 'student', 'students', 'expected', 'knowledges', 'method', 'methodology', 'methodologies', 'methods', 'problem', 'problems', 'model', 'models', 'modelling', 'project', 'projects', 'tutorial', 'tutorials', 'course', 'courses'}\n",
        "\n",
        "    # Our spaCy model\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    tokens = []\n",
        "    for courseDescription in nlp.pipe(df.iloc[:, 0]):\n",
        "        proj_tok = [token.lemma_.lower() for token in courseDescription if token.pos_ not in removal and not token.is_stop and token.is_alpha]\n",
        "        tokens.append(proj_tok) #I am saving the tokens in the db new column named tokens\n",
        "\n",
        "    df['tokens'] = tokens\n",
        "    return df\n",
        "\n",
        "# FUNCTION to generate the LDA visualization\n",
        "def generate_lda_model(df, num_topics, passes, no_below, no_above, random_seed):\n",
        "    # Clean the data\n",
        "    df = preprocess_data(df)\n",
        "\n",
        "    # Create the dictionary\n",
        "    tokens = df['tokens']\n",
        "    dictionary = Dictionary(tokens)\n",
        "\n",
        "    # Filter the dictionary\n",
        "    dictionary.filter_extremes(no_below=no_below, no_above=no_above, keep_n=None)\n",
        "\n",
        "    # Create the corpus\n",
        "    corpus = [dictionary.doc2bow(doc) for doc in tokens]\n",
        "\n",
        "    # Set the seed for the random number generator\n",
        "    random.seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "\n",
        "    # Train the LDA model\n",
        "    lda_model = LdaMulticore(corpus=corpus, id2word=dictionary, random_state=random_seed, num_topics=num_topics, passes=passes)\n",
        "\n",
        "    # Generate the LDA visualization\n",
        "    lda_display = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
        "    return  lda_model, lda_display\n",
        "\n",
        "\n",
        "# Define the widgets\n",
        "file_upload = widgets.FileUpload(accept='.xlsx', description='Upload Excel file containing one course/job description per row')\n",
        "num_topics = widgets.IntSlider(min=2, max=20, value=6, step=1, description='N Topics')\n",
        "passes = widgets.IntSlider(min=1, max=2000, value=10, step=1, description='N Passes')\n",
        "random_seed = widgets.IntSlider(min=1, max=10, value=5, step=1, description='Randomseed')\n",
        "no_below = widgets.IntSlider(min=1, max=10, value=3, step=1, description='Min count word')\n",
        "no_above = widgets.FloatSlider(min=0.1, max=1.0, step=0.1, value=0.9, description='Max count %')\n",
        "submit_button = widgets.Button(description='Save file and params')\n",
        "\n",
        "# Display the widgets\n",
        "display(file_upload)\n",
        "display(num_topics)\n",
        "display(passes)\n",
        "display(random_seed)\n",
        "display(no_below)\n",
        "display(no_above)\n",
        "display(submit_button)\n",
        "\n",
        "def on_submit_button_clicked(b):\n",
        "    # Read the data from the uploaded file into a pandas DataFrame\n",
        "    content = file_upload.value[next(iter(file_upload.value))]\n",
        "    df = pd.read_excel(io.BytesIO(content['content']))\n",
        "\n",
        "# Attach the function to the button click event\n",
        "submit_button.on_click(on_submit_button_clicked)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **STEP 3. Run the following code to display the Archetype in the form of topics extracted as clusters**"
      ],
      "metadata": {
        "id": "76Q7HTgOQnMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "content = file_upload.value[next(iter(file_upload.value))]\n",
        "df = pd.read_excel(io.BytesIO(content['content']))\n",
        "lda_model = generate_lda_model(df, num_topics.value, passes.value, no_below.value, no_above.value, random_seed.value)\n",
        "\n",
        "pyLDAvis.display(lda_model[1])"
      ],
      "metadata": {
        "cellView": "form",
        "id": "hGtjX5diKybu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **STEP 4. Download the Archetype in the form of 100 technical keywords (content) and Bloom Verbs (verbs) of each identified Topic.**\n",
        "*Note: Content can be e.g., retrived from ChatGPT by quering \"provide me a content of max 20 words for the topic which include these tehcnical keywords [...]\" and list the keywods*"
      ],
      "metadata": {
        "id": "357f1ky4dScj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "#create functions that I need\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from google.colab import files\n",
        "\n",
        "def is_not_bloomverb(word, bloom_verbs):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemma = lemmatizer.lemmatize(word, 'v')\n",
        "    return lemma not in bloom_verbs\n",
        "\n",
        "def remove_verb_forms(word_list):\n",
        "    verbs = set([word for synset in wordnet.all_synsets('v') for word in synset.lemma_names()])\n",
        "    infinitive_verbs = set()\n",
        "    for verb in verbs:\n",
        "        infinitive_verbs.update({lemma.name() for lemma in wordnet.lemmas(verb, 'v') if lemma.synset().pos() == 'v' and lemma.derivationally_related_forms() == []})\n",
        "    verbs_to_remove = verbs - infinitive_verbs\n",
        "    return [[word for word in sub_list if word not in verbs_to_remove] for sub_list in word_list]\n",
        "\n",
        "#Grab the first keywords 100 keywords\n",
        "KW=[]\n",
        "verb=[]\n",
        "bloomverbs = [\"understand\", \"define\", \"identify\", \"describe\", \"label\", \"list\", \"name\", \"state\", \"match\", \"recognize\", \"select\", \"examine\", \"locate\", \"memorize\", \"quote\", \"recall\", \"reproduce\", \"tabulate\", \"tell\", \"copy\", \"discover\", \"duplicate\", \"enumerate\", \"listen\", \"observe\", \"omit\", \"read\", \"recite\", \"record\", \"repeat\", \"retell\", \"visualize\", \"explain\", \"interpret\", \"paraphrase\", \"summarize\", \"classify\", \"compare\", \"differentiate\", \"discuss\", \"distinguish\", \"extend\", \"predict\", \"associate\", \"contrast\", \"convert\", \"demonstrate\", \"estimate\", \"express\", \"infer\", \"relate\", \"restate\", \"translate\", \"ask\", \"cite\", \"discover\", \"generalize\", \"group\", \"illustrate\", \"judge\", \"observe\", \"order\", \"report\", \"represent\", \"research\", \"review\", \"rewrite\", \"show\", \"trace\", \"solve\", \"apply\", \"modify\", \"use\", \"calculate\", \"change\", \"choose\", \"discover\", \"relate\", \"sketch\", \"complete\", \"construct\", \"interpret\", \"manipulate\", \"paint\", \"prepare\", \"teach\", \"act\", \"compute\", \"list\", \"practice\", \"simulate\", \"write\", \"analyze\", \"classify\", \"contrast\", \"infer\", \"select\", \"categorize\", \"connect\", \"differentiate\", \"estimate\", \"evaluate\", \"focus\", \"organize\", \"plan\", \"question\", \"test\", \"reframe\", \"criticize\", \"appraise\", \"support\", \"decide\", \"recommend\", \"assess\", \"convince\", \"defend\", \"grade\", \"predict\", \"select\", \"argue\", \"conclude\", \"critique\", \"debate\", \"justify\", \"persuade\", \"weigh\", \"design\", \"compose\", \"plan\", \"combine\", \"formulate\", \"invent\", \"substitute\", \"compile\", \"develop\", \"integrate\", \"modify\", \"prepare\", \"rearrange\", \"adapt\", \"arrange\", \"collaborate\", \"facilitate\", \"make\", \"manage\", \"propose\", \"solve\", \"support\", \"test\", \"validate\"]\n",
        "\n",
        "for nTopic in range(num_topics.value):\n",
        "  KW.append([])\n",
        "  for a in range(100):\n",
        "    word = lda_model[0].show_topic(nTopic, topn=100)[a][0]\n",
        "    if is_not_bloomverb(word, bloomverbs):\n",
        "      KW[nTopic].append(word)\n",
        "    else:\n",
        "      KW[nTopic].append('*' + word + '*')\n",
        "\n",
        "KW_no_verb = remove_verb_forms(KW)\n",
        "KW_no_verb_df = pd.DataFrame({'Technical content and bloom verb in bolded font': KW_no_verb})\n",
        "\n",
        "#save in a table\n",
        "\n",
        "KW_no_verb_df.to_excel(r'ArchetypeFinal.xlsx', index=False)\n",
        "files.download('ArchetypeFinal.xlsx')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "c4FdPVLDdR4k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}